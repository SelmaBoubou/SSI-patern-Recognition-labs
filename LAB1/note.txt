# Linear regression with multiple variables
def linear_regression_mutiple (x,y):
    print('there is {} samples and {} features'.format(X.shape[0], X.shape[1]))

#check that the linear regression is working 
w = linear_regression_multiple(X,y)
# w should have three parameters now
assert w.size == 3
print ('The parameters estimated are : {}'.format(w))




#my gradient fucntion

# function gradient 
def GradientDescent(X, y, alpha, NIter):
    #initialize w
    w = np.zeros(size(X,2)+1, NIter)
    #Add a column of ones to the features
    X = np.ones((size(X,1),1),X)
    #iterate until convergence
    for i in range (1,NIter):
        suma = 0;
        #for every feature:
        for j in range (1,size(X,2)):
            #Compute the weight update:
            if i>1:
                suma= sum((X*w[:,i-1] - y)*X[:,j])
            else:
                suma= sum((X*w[:,i] - y)*X[:,j])
            return
        
        deriv=suma/size(X,1)
        u[j] = alpha*deriv
        #Correct the weights according to the update:
        if i>1:
            w[:,i] = w[:,i-1] - u
        else:


#-------------------------------------------------------------------    
X = data[:, 0]
y = data[:, 1]
#Some gradient descent settings
iterations = 1500
alpha = 0.01
# plotting

theta, J_history = gradient_descent(it, y, theta, alpha, iterations)
plt.figure()
plt.plot(J_history)
plt.show()

                w[:,i] = - u
        return (u)

# check that Gradient descent function





def costFunction(X):
    #Copmute the cost function:
    suma=0;
    for i in range (1,size(X,1)):
        suma=suma + (X[i,:]*w[:,i] - y[i,:])**2
        return (suma)
    J[i]=suma/(2*size(X,1))
    return (J[i])
# showing the answer
#a = costFunction(J)
print('The parameters estimated are: {}'.format(X))